# DQN Training Configuration
# Usage: Modify these values and pass to training script

# Environment
environment:
  room: 0                    # Level number (0-30)
  max_steps: 500             # Max steps per episode
  use_simple_actions: true   # Use reduced action space (15 actions)

# Training
training:
  num_episodes: 3000         # Total training episodes
  log_interval: 50           # Episodes between logging
  save_interval: 200         # Episodes between checkpoints

# Agent Hyperparameters
agent:
  learning_rate: 0.0005      # Adam optimizer learning rate
  gamma: 0.99                # Discount factor
  epsilon_start: 1.0         # Initial exploration rate
  epsilon_end: 0.05          # Final exploration rate
  epsilon_decay: 0.9995      # Exploration decay per step
  batch_size: 128            # Training batch size
  buffer_size: 200000        # Replay buffer capacity
  target_update_freq: 200    # Steps between target network updates

# Network Architecture
network:
  hidden_dim: 256            # Size of hidden layers
  num_layers: 3              # Number of hidden layers

# Reward Shaping
rewards:
  death_penalty: -5.0        # Penalty for dying
  completion_bonus: 100.0    # Bonus for completing level
  height_bonus_scale: 1.0    # Multiplier for height progress
  movement_bonus: 0.01       # Bonus for any movement
  stuck_penalty: -0.1        # Penalty for being stuck
  time_penalty: -0.01        # Per-step time penalty
